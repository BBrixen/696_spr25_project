CEO Sundar Pichai’s 11 Top Remarks At Google I/O 2021 | CRN CEO Sundar Pichai’s 11 Top Remarks At Google I/O 2021 ‘We believe LaMDA’s natural conversation capabilities have the potential to make information and computing radically more accessible and easier to use,’ Pichai said. New Google artificial intelligence innovations including LaMDA natural language technology, a faster, “historic milestone” chip designed specifically for machine learning and next-generation, 3-D videoconferencing were among the products and projects unveiled by Sundar Pichai at Google I/O this week. Pichai, CEO of Alphabet and its Google subsidiary , also heralded Google’s new Quantum AI computing campus in Santa Barbara, Calif. and the company’s sustainability goals, which will be aided by a geothermal project that will add carbon-free energy to the electric grid serving its Nevada data centers and infrastructure. Google I/O is the technology company’s annual developers conference that runs virtually through Thursday after being cancelled outright last year due to coronavirus-related restrictions. “I think of I/O not just as a celebration of technology, but of the people who use it and build it, including the millions of developers watching today,” Pichai said, speaking live and outdoors from Google’s campus in Mountain View, Calif. “Over the past year, we’ve seen how technology can be used to help billions of people through the most difficult of times. It’s made us more committed than ever to our goal of building a more helpful Google for everyone.” Here’s a look at Pichai’s top remarks from his keynote address at Google I/O, which also covered highlights of new Google Maps features and Google’s quantum computing goal of creating an error-corrected logical qubit. New Google Maps Features At Google, the past year has given renewed purpose to our mission to organize the world’s information and make it universally accessible and useful. We continue to approach that mission with a singular goal: Building a more helpful Google for everyone. That means being helpful in moments that matter, and it means giving you the tools to increase your knowledge, success, health and happiness. Sometimes, it’s about helping in little moments that add up to big changes. Recently, we added 150,000 kilometers of bike lanes in Google Maps. We’re also introducing two new features. First, new eco-friendly routes. Using our understanding of road and traffic conditions, Google Maps will soon give you the option to take the most fuel-efficient route. At scale, this has potential to significantly reduce car emissions and fuel consumption. Second, safer routing. Powered by AI, Maps can identify road, weather and traffic conditions where you’re likely to have to suddenly brake. We aim to reduce up to 100 million of these events every year. AI Advances In Translation And Image Recognition Google Search was built on the insight that understanding links between webpages leads to dramatically better search results. We’ve made remarkable advances over the past 22 years, and Search helps billions of people. And to improve Search even further, we need to deepen our understanding of language and context. To do this requires advances in the most challenging areas of AI, and I want to talk about a few today, starting with translation. We learn and understand knowledge best in our native languages. So, 15 years ago, we set out to translate the web -- an incredibly ambitious goal at the time. Today, hundreds of millions of people use Google Translate each month across more than 100 languages. Last month alone, we translated more than 20 billion web pages in Chrome. With Google Assistant’s interpreter mode, you can have a conversation with someone speaking a foreign language, and usage is up four times from just a year ago. While there is still work to do, we are getting closer to having a universal translator in your pocket. At the same time, advances in machine learning have led to tremendous breakthroughs in image recognition. In 2014, we first trained a production system to automatically label images -- a step change in computers’ understanding of visual information -- and it allowed us to imagine and launch Google Photos. Today, we can surface and share a memory, reminding you of some of the best moments in your life. Last month alone, more than 2 billion memories were viewed. Image recognition also means you can use Google Lens to take a photo of a math problem. I wish I had this when I was in school. Lens is used more than 3 billion times each month. We can now be as helpful with images as we are with text. dell machine learning Machine Learning And Voice Technology Machine learning has also improved how computers comprehend and communicate with human voices. That’s why we can caption conversations in Google Meet and why Live Caption on Android can automatically caption anything running on your smartphone locally. It generates 250,000 hours of captioning every day. The breakthrough technology from (UK-based Alphabet subsidiary) DeepMind called WaveNet increased the quality of computer-generated speech, leading to more natural and fluid interactions. WaveNet allowed us to create and deploy 51 voices across Google Assistant. Natural Language Processing Together, the advances in AI…across translation, images and voice improved the search experience for billions of people. They also enabled us to make a huge leap forward in how computers process natural language. In 2017, we first introduced the world to Transformers, a novel machine learning approach for better natural language understanding. Transformers became the foundation for many other breakthroughs, like (DeepMind’s) AlphaFold and BERT (Bidirectional Encoder Representations from Transformers), which we introduced in 2019. BERT considers the full context of a word by looking at the words that come before and after, leading to one of our most significant quality improvements across Google Search and enabling us to respond to queries with more helpful answers. The richness and flexibility of language make it one of humanity’s greatest tools and one of computer sciences’ greatest challenges. If someone asks me if the temperature in the room is okay, and I say, “I am freezing,” they know that I’m very cold, not literally freezing. Or if someone says, “What’s the weather like today?” I don’t respond, “It’s 70 degrees sunny and a 12 percent chance of rain.” I’d probably say, “It’s starting to feel like summer, I might eat lunch outside.” Sensible responses keep conversations going and allow them to end up in a completely different place from where they started. Even if I began by talking about the weather, we may end up talking about football. LaMDA: ‘Huge Step Forward In Natural Conversation’ Today, I’m excited to share our latest breakthrough in natural language understanding: LaMDA. It’s a language model for dialog applications, and it’s open domain, which means it’s designed to converse on any topic. And while it’s still in research and development, we’ve been using it internally to explore novel interactions. For example, say you wanted to learn about one of my favorite planets, Pluto. LaMDA already understands quite a lot about Pluto and millions of other topics. (Google proceeds to show a video conversation that one of its teams had with “Pluto” at the 18:35 mark here .) I spent some time with my son conversing with Pluto, and it’s magical. We had a lot of fun learning about space together. Let’s break down what made it feel so natural. First, learned concepts. As you saw, the model talked about the New Horizons spacecraft and the coldness of space. LaMDA synthesized these concepts from its training data. These concepts were not hand-programmed in the model. Because none of the responses were predefined, LaMDA answered with sensible responses, keeping the dialogue open-ended. Natural conversations are generative, and they never take the same path twice. And LaMDA is able to carry a conversation no matter what we talk about. You can have another conversation without retraining the model. It’s really impressive to see how LaMDA can carry on a conversation about any topic. It’s amazing how sensible and interesting the conversation is. Yet, it’s still early research, so it doesn’t get everything right. Sometimes, it can give nonsensical responses, imagining Pluto doing flips or playing fetch with its favorite ball, the moon. Other times, it just doesn’t keep the conversation going. At Google, we’ve been researching and developing language models for many years. We are focused on ensuring LaMDA meets our incredibly high standards on fairness, accuracy, safety and privacy. So, from concept all the way to design, we are making sure it’s developed consistent with our AI principles. We believe LaMDA’s natural conversation capabilities have the potential to make information and computing radically more accessible and easier to use. We look forward to incorporating better conversational features into products like Google Assistant, Search and Workspace. We’re also exploring how to give capabilities to developers and enterprise customers. LaMDA is a huge step forward in natural conversation, but it is still trained only on text. When people communicate with each other, they do it across images, text, audio and video. So we need to build models that allow people to naturally ask questions across different types of information. These are called multimodal models. Let’s say we want a model to recognize all facets of a road trip. That could mean the words “road trip” written or spoken in any language, images, sounds and videos and concepts associated with road trips, such as weather and directions. So, you can imagine one day planning a road trip and asking Google to find a route with beautiful mountain views. You can also use this to search for something within a video. For example, when you say, “Show me the part where the lion roars at sunset,” we will get you to that exact moment in a video. Translation, image recognition, voice recognition, text-to-speech, Transformers -- all of this work laid the foundation for complex models like LaMDA and multimodal. New TPU v4 Chip For Machine Learning Our compute infrastructure is how we drive and sustain these advances, and Tensor Processing Units (TPUs) are a big part of that. Today, I’m excited to announce our next generation, the TPU v4. These are powered by the v4 chip, which is more than twice as fast as the v3 chip (introduced in 2018). TPUs are connected together into supercomputers called pods. A single v4 pod contains 4,096 v4 chips, and each pod has 10x the interconnect bandwidth per chip at scale compared to any other networking technology. This makes it possible for a TPU v4 pod to deliver more than one exaFLOP -- 10 to the 18th power floating point operations per second of computing power. Think about it this way: If 10 million people were on their laptops right now, then all of those laptops put together would almost match the computing power of one exaFLOP. This is the fastest system we’ve ever deployed at Google, an historic milestone for us. Previously, to get an exaFLOP, you needed to build a custom supercomputer. But we already have many of these deployed today, and we’ll soon have dozens of TPU v4 pods in our data centers, many of which will be operating at or near 90 percent carbon-free energy. And our TPU v4 pods will be available to our cloud customers later this year. It’s tremendously exciting to see this pace of innovation. Quantum And An Error-Corrected Logical Qubit As we look further into the future, there are types of problems that classical computing will not be able to solve in a reasonable time. Quantum computing represents a fundamental shift, because it harnesses the properties of quantum mechanics and gives us the best chance of understanding the natural world. Achieving our quantum milestone was a tremendous accomplishment, but we are still at the very beginning of a multi-year journey. One problem we face today is that our physical qubits are very fragile. Even cosmic rays from outer space can destroy quantum information. To solve more complex problems, our next milestone is to create an error-corrected logical qubit. It’s simply a collection of physical qubits stable enough to hold quantum information for a long period of time. We start by reducing the error rate of our physical qubits, then combining 1,000 physical qubits to create a single logical qubit, and then scaling that up to 1000 logical qubits, at which point, we will have created an error-corrected quantum computer. Today, we are focused on enabling scientists and developers to access beyond-classical computational resources. But we hope to one day create an error-corrected quantum computer, and success could mean everything from increasing battery efficiency, to creating more sustainable energy, to improved drug discovery and so much more. The roadmap begins in our new data center, which we are calling the Quantum AI campus (in Santa Barbara, Calif.). We recognize that building an error-corrected quantum computer will be incredibly challenging. But solving hard problems and advancing the state of the art is how we build the most helpful products. Auto-Delete At Google, we know that our products can only be as helpful as they are safe, and advances in computer science and AI are how we continue to make them better. We keep more users safe by blocking malware, phishing attempts, spam messages and potential cyberattacks than anyone else in the world. And our focus on data minimization pushes us to do more with that. Two years ago at I/O, I announced auto-delete, which encourages users to have their activity data automatically and continuously deleted. We have since made auto-delete the default for all new Google accounts. Now, after 18 months, we automatically delete your data, unless you tell us to do it sooner. And this is now active for over 2 billion accounts. Project Starline We were all grateful to have videoconferencing over the last year. It helped us stay in touch with family and friends, and kept businesses and schools going. But there is no substitute for being together in the room with someone. So, several years ago, we kicked off a project to use technology to explore what’s possible. We call it Project Starline. (The technology project enables people to “feel together, even when they’re cities or countries apart,” according to Google. The company likened it to looking though a “magic window” and seeing another life-size person in three dimensions and being able to talk naturally, gesture and make eye contact.) It builds on the different areas of computer science I spoke about today and relies on custom-built hardware and highly specialized equipment. It’s early and currently available in just a few of our offices. Some key advances have made this experience possible. First, using high-resolution cameras and custom-built depth sensors, we capture your shape and appearance from multiple perspectives, and then fuse them together to create an extremely detailed, real-time 3D model. The resulting data is huge -- many gigabits per second. To send this 3D imagery over existing networks, we developed novel compression and streaming algorithms that reduce the data by a factor of more than 100. And we have developed a breakthrough light field display that shows you the realistic representation of someone sitting right in front of you in three dimensions. As you move your head and body, our system adjusts the images to match your perspective. You can talk naturally, gesture and make eye contact. It’s as close as we can get to the feeling of sitting across from someone. As sophisticated as the technology is, it vanishes, so you can focus on what’s most important. With Project Starline , we have brought together a set of advanced technologies, with the goal of creating the best communications experience possible. We have spent thousands of hours testing it in our own offices, and the results are promising. There’s also excitement from our lead enterprise partners. We plan to expand access to partners in healthcare and media. In pushing the boundaries of remote collaboration, we have made technical advances that will improve our entire suite of communications products. We look forward to sharing more ways for you to get involved in the months ahead. Sustainability Efforts Sustainability has been a core value for more than 20 years. We were the first major company to become carbon-neutral in 2007. We were also the first to match our operations with 100 percent renewable energy. That was in 2017, and we’ve been doing it ever since. And last year, we eliminated our entire carbon legacy. Our next ambition is our biggest yet. By 2030, we aim to operate on carbon-free energy 24/7. This means running every data center and office on clean electricity every hour of every day. Operating 24/7 on carbon-free energy is a step change from current approaches. It means setting a higher bar to never emit carbon from our operations in the first place. It’s a moon shot like LaMDA or quantum computing, and it presents an equally hard set of problems to solve. First, we have to source carbon-free energy in every place we operate -- a harder task in some places than in others. Today, five of our data centers are already operating at or near 90 percent carbon-free energy. In Denmark, we built five new solar farms to support our newest data center, complementing existing wind energy on the Danish grid. And it’s operated carbon-free 90 percent of the time since day one. Another challenge of 24/4 carbon-free energy is just that-- it has to run every hour of every day. So last year, we rolled out the world’s first carbon-intelligent computing platform. It automatically shifts the timing of many compute tasks to when clean power sources are most plentiful. And today I’m excited to announce we are the first company to implement carbon-intelligent load-shifting across both time and place within our data center network. By this time next year, we’ll be shifting more than a third of non-production compute to times and places with greater availability of carbon-free energy. network servers racks with light,3D physically rending high quality. Google And Geothermal Energy Projects To reach 24/7 (carbon-free energy), we also need to go beyond wind and solar, and tap into sources of on-demand clean energy like geothermal. Geothermal uses the consistent heat from the earth to generate electricity, but it’s not widely used today, and we want to change that. I’m excited to announce that we are partnering to develop a next-generation geothermal power project. This will connect to the grid, serving our Nevada data centers, starting next year. We believe our cloud AI, combined with the partners’ expertise in fiber optic sensing and novel techniques can unlock flexible geothermal power in a broad range of new places. Investments like these are needed to get to 24/7 carbon-free energy, and it’s happening right here in Mountain View, too. We are building our new campus to the highest sustainability standards. When completed, these buildings will feature first-of-its-kind, dragonscale solar skin equipped with 90,000 silver solar panels and the capacity to generate nearly seven megawatts. They will house the largest geothermal pile system in North America, helping to heat the buildings in the winter and cool them in the summer. Sustainability is one of the defining challenges of our time, and advances in computer science and AI have a huge role to play in meeting it.