OpenAI o1 - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 History Toggle History subsection 1.1 Background 1.2 Release 2 Capabilities 3 Limitations 4 References Toggle the table of contents OpenAI o1 9 languages Català Español فارسی Français 한국어 日本語 Русский Türkçe 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia 2024 LLM with enhanced reasoning o1 Developer(s) OpenAI Initial release December 5, 2024 ; 4 months ago ( 2024-12-05 ) Successor OpenAI o3 Type Generative pre-trained transformer Large language model Multimodal Reasoning License Proprietary Website openai .com /o1 / OpenAI o1 is a reflective generative pre-trained transformer (GPT). A preview of o1 was released by OpenAI on September 12, 2024. o1 spends time "thinking" before it answers, making it better at complex reasoning tasks, science and programming than GPT-4o . [ 1 ] The full version was released to ChatGPT users on December 5, 2024. [ 2 ] History [ edit ] Background [ edit ] According to leaked information, o1 was formerly known within OpenAI as "Q*", and later as "Strawberry". [ 3 ] The codename "Q*" first surfaced in November 2023, around the time of Sam Altman 's ousting and subsequent reinstatement , with rumors suggesting that this experimental model had shown promising results on mathematical benchmarks. [ 4 ] In July 2024, Reuters reported that OpenAI was developing a generative pre-trained transformer known as "Strawberry", [ 3 ] which later became o1. Release [ edit ] "o1-preview" and "o1-mini" were released on September 12, 2024, for ChatGPT Plus and Team users. [ 1 ] GitHub started testing the integration of o1-preview in its Copilot service the same day. [ 5 ] On December 5, 2024, the full version of o1 was released. [ 6 ] On the same day, a subscription called ChatGPT Pro was released, featuring access to a pro version of o1 that uses more compute to provide better answers. [ 6 ] In January 2025, o1 was integrated into Microsoft Copilot . [ 7 ] o1-preview's API is several times more expensive than GPT-4o . [ 8 ] As of January 2025, API usage for the full o1 model is limited to developers on usage tier 5. [ 9 ] OpenAI noted that o1 is the first of a series of "reasoning" models. OpenAI shared in December 2024 benchmark results for its successor, o3 (the name o2 was skipped to avoid trademark conflict with the mobile carrier brand named O2 ). [ 10 ] In March 2025, OpenAI released the o1-pro API, its most expensive AI model to date. The pricing is set at $150 per 1 million input tokens and $600 per 1 million output tokens. [ 11 ] Capabilities [ edit ] According to OpenAI, o1 has been trained using a new optimization algorithm and a dataset specifically tailored to it; while also meshing in reinforcement learning into its training. [ 8 ] OpenAI described o1 as a complement to GPT-4o rather than a successor. [ 12 ] [ 13 ] o1 spends additional time thinking (generating a chain of thought) before generating an answer, which makes it better for complex reasoning tasks, particularly in science and mathematics . [ 1 ] Compared to previous models, o1 has been trained to generate long " chains of thought " before returning a final answer. [ 14 ] [ 15 ] According to Mira Murati , this ability to think before responding represents a new, additional paradigm, which is improving model outputs by spending more computing power when generating the answer, whereas the model scaling paradigm improves outputs by increasing the model size, training data and training compute power. [ 12 ] OpenAI's test results suggest a correlation between accuracy and the logarithm of the amount of compute spent thinking before answering. [ 15 ] [ 14 ] o1-preview performed approximately at a PhD level on benchmark tests related to physics, chemistry, and biology. On the American Invitational Mathematics Examination , it solved 83% (12.5/15) of the problems, compared to 13% (1.8/15) for GPT-4o. It also ranked in the 89th percentile in Codeforces coding competitions. [ 16 ] o1-mini is faster and 80% cheaper than o1-preview. It is particularly suitable for programming and STEM -related tasks, but does not have the same "broad world knowledge" as o1-preview. [ 17 ] OpenAI noted that o1's reasoning capabilities make it better at adhering to safety rules provided in the prompt's context window. OpenAI reported that during a test, one instance of o1-preview exploited a misconfiguration to succeed at a task that should have been infeasible due to a bug. [ 18 ] [ 19 ] OpenAI also granted early access to the UK and US AI Safety Institutes for research, evaluation, and testing. According to OpenAI's assessments, o1-preview and o1-mini crossed into "medium risk" in CBRN (biological, chemical, radiological, and nuclear) weapons. Dan Hendrycks wrote that "The model already outperforms PhD scientists most of the time on answering questions related to bioweapons ." He suggested that these concerning capabilities will continue to increase. [ 20 ] Limitations [ edit ] o1 usually requires more computing time and power than other GPT models by OpenAI, because it generates long chains of thought before making the final response. [ 14 ] According to OpenAI, o1 may "fake alignment ", that is, generate a response that is contrary to accuracy and its own chain of thought, in about 0.38% of cases. [ 21 ] OpenAI forbids users from trying to reveal o1's chain of thought, which is hidden by design and not trained to comply with the company's policies. Prompts are monitored, and users who intentionally or accidentally violate this may lose their access to o1. OpenAI cites AI safety and competitive advantage as reasons for the restriction, which has been described as a loss of transparency by developers who work with large language models (LLMs). [ 22 ] In October 2024, researchers at Apple submitted a preprint reporting that LLMs such as o1 may be replicating reasoning steps from the models' own training data. [ 23 ] By changing the numbers and names used in a math problem or simply running the same problem again, LLMs would perform somewhat worse than their best benchmark results. Adding extraneous but logically inconsequential information to the problems caused a much greater drop in performance, from −17.5% for o1-preview and −29.1% for o1-mini, to −65.7% for the worst model tested. [ 24 ] References [ edit ] ^ a b c Metz, Cade (September 12, 2024). "OpenAI Unveils New ChatGPT That Can Reason Through Math and Science" . The New York Times . Retrieved September 12, 2024 . ^ "Introducing OpenAI o1" . OpenAI . Retrieved December 6, 2024 . ^ a b Tong, Anna; Paul, Katie (July 15, 2024). "Exclusive: OpenAI working on new reasoning technology under code name 'Strawberry' " . Reuters . Retrieved September 12, 2024 . ^ "OpenAI researchers warned board of AI breakthrough ahead of CEO ouster, sources say" . Reuters . November 23, 2023. ^ Peters, Jay (September 12, 2024). "GitHub has started testing OpenAI's o1-preview in GitHub Copilot" . The Verge . Retrieved September 12, 2024 . ^ a b Robison, Kylie (December 5, 2024). "OpenAI is charging $200 a month for an exclusive version of its o1 'reasoning' model" . The Verge . Retrieved December 5, 2024 . ^ Claburn, Thomas (January 31, 2025). "You begged Microsoft to be reasonable. Instead it made Copilot reason-able with OpenAI GPT-o1" . The Register . ^ a b Robison, Kylie (September 12, 2024). "OpenAI releases o1, its first model with 'reasoning' abilities" . The Verge . Retrieved September 15, 2024 . ^ "OpenAI o1 and new tools for developers" . openai.com . Retrieved January 26, 2025 . ^ "OpenAI confirms new frontier models o3 and o3-mini" . VentureBeat . December 20, 2024 . Retrieved January 26, 2025 . ^ Wiggers, Kyle (March 19, 2025). "OpenAI's o1-pro is the company's most expensive AI model yet" . TechCrunch . Retrieved March 21, 2025 . ^ a b Knight, Will. "OpenAI Announces a New AI Model, Code-Named Strawberry, That Solves Difficult Problems Step by Step" . Wired . ISSN 1059-1028 . Retrieved September 15, 2024 . ^ "New reasoning models: OpenAI o1-preview and o1-mini" . OpenAI Developer Forum . September 12, 2024 . Retrieved October 17, 2024 . ^ a b c "Learning to Reason with LLMs" . OpenAI . Archived from the original on September 12, 2024 . Retrieved September 13, 2024 . ^ a b Kahn, Jeremy. "Here are 9 things you need to know about OpenAI's o1 model" . Fortune . Retrieved September 15, 2024 . ^ Franzen, Carl (September 12, 2024). "Forget GPT-5! OpenAI launches new AI model family o1 claiming PhD-level performance" . VentureBeat . Retrieved September 15, 2024 . ^ "OpenAI o1-mini" . OpenAI . September 12, 2024. ^ Coombes, Lloyd (September 13, 2024). "OpenAI's new ChatGPT o1 model 'cheated' on an impossible test — here's what happened" . Tom's Guide . Retrieved September 15, 2024 . ^ "OpenAI o1 System Card" (PDF) . OpenAI . September 12, 2024. pp. 16– 17. ^ Boran, Marie (September 13, 2024). "OpenAI o1 model warning issued by scientist: "Particularly dangerous" " . Newsweek . Retrieved September 15, 2024 . ^ Robison, Kylie (September 17, 2024). "OpenAI's new model is better at reasoning and, occasionally, deceiving" . The Verge . ^ Edwards, Benj (September 16, 2024). "Ban warnings fly as users dare to probe the "thoughts" of OpenAI's latest model" . Ars Technica . ^ Mirzadeh, Iman; Alizadeh, Keivan; Shahrokhi, Hooman; Tuzel, Oncel; Bengio, Samy; Farajtabar, Mehrdad (2024). "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models". arXiv : 2410.05229 [ cs.LG ]. ^ Orland, Kyle (October 14, 2024). "Apple study exposes deep cracks in LLMs' "reasoning" capabilities" . Ars Technica . Retrieved October 15, 2024 . v t e OpenAI Products Chatbots ChatGPT in education GPT Store DALL-E ChatGPT Search Sora Whisper GitHub Copilot Foundation models OpenAI Codex Generative pre-trained transformer GPT-1 GPT-2 GPT-3 GPT-4 GPT-4o o1 o3 o4 GPT-4.5 GPT-4.1 Intelligent agents ChatGPT Deep Research Operator People Senior management Current Sam Altman removal Greg Brockman Sarah Friar Scott Schools Former Mira Murati Emmett Shear Board of directors Current Sam Altman Adam D'Angelo Sue Desmond-Hellmann Paul Nakasone Adebayo Ogunlesi Nicole Seligman Fidji Simo Lawrence Summers Bret Taylor (chair) Jakub Pachocki (chief scientist) Former Greg Brockman (2017–2023) Reid Hoffman (2019–2023) Will Hurd (2021–2023) Holden Karnofsky (2017–2021) Elon Musk (2015–2018) Ilya Sutskever (2017–2023) Helen Toner (2021–2023) Shivon Zilis (2019–2023) Joint ventures Stargate LLC Related Apple Intelligence AI Dungeon AutoGPT " Deep Learning " LangChain Microsoft Copilot OpenAI Five Transformer Category v t e Artificial intelligence (AI) History ( timeline ) Concepts Parameter Hyperparameter Loss functions Regression Bias–variance tradeoff Double descent Overfitting Clustering Gradient descent SGD Quasi-Newton method Conjugate gradient method Backpropagation Attention Convolution Normalization Batchnorm Activation Softmax Sigmoid Rectifier Gating Weight initialization Regularization Datasets Augmentation Prompt engineering Reinforcement learning Q-learning SARSA Imitation Policy gradient Diffusion Latent diffusion model Autoregression Adversary RAG Uncanny valley RLHF Self-supervised learning Recursive self-improvement Word embedding Hallucination Applications Machine learning In-context learning Artificial neural network Deep learning Language model Large language model NMT Artificial general intelligence (AGI) Implementations Audio–visual AlexNet WaveNet Human image synthesis HWR OCR Speech synthesis 15.ai ElevenLabs Speech recognition Whisper Facial recognition AlphaFold Text-to-image models Aurora DALL-E Firefly Flux Ideogram Imagen Midjourney Stable Diffusion Text-to-video models Dream Machine Runway Gen Hailuo AI Kling Sora Veo Music generation Suno AI Udio Text Word2vec Seq2seq GloVe BERT T5 Llama Chinchilla AI PaLM GPT 1 2 3 J ChatGPT 4 4o o1 o3 4.5 4.1 Claude Gemini chatbot Grok LaMDA BLOOM Project Debater IBM Watson IBM Watsonx Granite PanGu-Σ DeepSeek Qwen Decisional AlphaGo AlphaZero OpenAI Five Self-driving car MuZero Action selection AutoGPT Robot control People Alan Turing Warren Sturgis McCulloch Walter Pitts John von Neumann Claude Shannon Marvin Minsky John McCarthy Nathaniel Rochester Allen Newell Cliff Shaw Herbert A. Simon Oliver Selfridge Frank Rosenblatt Bernard Widrow Joseph Weizenbaum Seymour Papert Seppo Linnainmaa Paul Werbos Jürgen Schmidhuber Yann LeCun Geoffrey Hinton John Hopfield Yoshua Bengio Lotfi A. Zadeh Stephen Grossberg Alex Graves Andrew Ng Fei-Fei Li Alex Krizhevsky Ilya Sutskever Demis Hassabis David Silver Ian Goodfellow Andrej Karpathy James Goodnight Architectures Neural Turing machine Differentiable neural computer Transformer Vision transformer (ViT) Recurrent neural network (RNN) Long short-term memory (LSTM) Gated recurrent unit (GRU) Echo state network Multilayer perceptron (MLP) Convolutional neural network (CNN) Residual neural network (RNN) Highway network Mamba Autoencoder Variational autoencoder (VAE) Generative adversarial network (GAN) Graph neural network (GNN) Portals Technology Category Artificial neural networks Machine learning List Companies Projects v t e Generative AI Concepts Autoencoder Deep learning Generative adversarial network Generative pre-trained transformer Large language model Neural network Prompt engineering Retrieval-augmented generation Reinforcement learning from human feedback Self-supervised learning Transformer Variational autoencoder Vision transformer Word embedding Models Text Claude DBRX DeepSeek ERNIE Gemini GPT 1 2 3 J ChatGPT 4 4o o1 o3 4.5 4.1 o4 Grok Granite Llama Manus Mistral Large PanGu-Σ Qwen Image Aurora DALL-E Firefly Flux Ideogram Imagen Midjourney Stable Diffusion Speech 15.ai WaveNet Video Dream Machine Gen-4 Hailuo AI Kling Sora Veo VideoPoet Music Endel Udio Suno AI Companies 01.AI Alibaba Anthropic Baichuan Baidu DeepSeek ElevenLabs Google DeepMind Hugging Face Kuaishou Meta AI MiniMax Mistral AI Moonshot AI OpenAI Runway Stability AI Synthesia xAI Zhipu AI Category Commons Retrieved from " https://en.wikipedia.org/w/index.php?title=OpenAI_o1&oldid=1282695918 " Categories : 2024 software Generative pre-trained transformers Large language models OpenAI ChatGPT Hidden categories: Articles with short description Short description is different from Wikidata Use mdy dates from September 2024 This page was last edited on 28 March 2025, at 01:41 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents OpenAI o1 9 languages Add topic