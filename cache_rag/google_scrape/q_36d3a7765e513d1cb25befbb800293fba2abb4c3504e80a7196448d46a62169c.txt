OpenAI Announces GPT-3 AI Language Model with 175 Billion Parameters - InfoQ BT InfoQ Software Architects' Newsletter A monthly overview of things you need to know as an architect or aspiring architect. View an example Enter your e-mail address Select your country Select a country I consent to InfoQ.com handling my data as explained in this Privacy Notice . We protect your privacy. Close Toggle Navigation Facilitating the Spread of Knowledge and Innovation in Professional Software Development English edition English edition Chinese edition Japanese edition French edition Write for InfoQ Search Sign Up / Login Email Password Forgot password ? InfoQ Account Email Back to login Resend Activation Back to login Login with: Google Microsoft Twitter Facebook Don't have an InfoQ account? Sign Up Logo - Back to homepage News Articles Presentations Podcasts Guides Topics Development Java Kotlin .Net C# Swift Go Rust JavaScript Featured in  Development Quantum Shift: Rewiring the Tech Landscape Teena Idnani explains the core concepts of quantum computing, its transformative potential across industries (pharmaceuticals, finance, etc.), and the current state of quantum technology. She addresses the threats (breaking RSA encryption) and opportunities, offering practical guidance on how software developers, architects, and engineering leaders can prepare for the coming quantum revolution. All in  development Architecture & Design Architecture Enterprise Architecture Scalability/Performance Design Case Studies Microservices Service Mesh Patterns Security Featured in  Architecture & Design InfoQ Software Architecture and Design Trends Report - 2025 The InfoQ Trends Reports offer InfoQ readers a comprehensive overview of key topics worthy of attention. The reports also guide the InfoQ editorial team towards cutting-edge technologies in our reporting. In conjunction with the report and trends graph, our accompanying podcast features insightful discussions among the editors digging deeper into some of the trends. All in  architecture-design AI Infrastructure Big Data Machine Learning NoSQL Database Data Analytics Streaming Featured in  AI, ML & Data Engineering From "Simple" Fine-Tuning to Your Own Mixture of Expert Models Using Open-Source Models Sebastiano Galazzo discusses the practicalities of creating custom LLMs and shares real-world tips and common mistakes. Learn cost-effective fine-tuning (LoRA), powerful model merging ("Franken models"), Mixture of Experts, multimodal capabilities & key performance optimizations (pruning, quantization) for maximum value and relevance to senior software developers and engineering leaders. All in  ai-ml-data-eng Culture & Methods Agile Diversity Leadership Lean/Kanban Personal Growth Scrum Sociocracy Software Craftmanship Team Collaboration Testing UX Featured in  Culture & Methods A Game of Patterns Tiani Jones explains how recognizing organizational behaviors and their resulting patterns can reveal opportunities for improvement and lead to better performance. Using examples like chess and a cyber-physical team, she shares practical insights on identifying anti-patterns and fostering generative cultures through mindful adjustments to constraints and practices in software development. All in  culture-methods DevOps Infrastructure Continuous Delivery Automation Containers Cloud Observability Featured in  DevOps Implement the EU Cyber Resilience Act's Requirements to Strengthen Your Software Project Eddie Knight, OSPO lead at Sonatype, discusses how the EU Cyber Resilience Act can help with improving your software project’s security and in the same time to slow down the alarming acceleration of software supply chain attacks. All in  devops Events Helpful links About InfoQ InfoQ Editors Write for InfoQ About C4Media Diversity Choose your language En 中文 日本 Fr InfoQ Dev Summit Boston Learn how senior software developers are solving the challenges you face. Register now with early bird tickets. InfoQ Dev Summit Munich Learn practical solutions to today's most pressing software challenges. Register now with early bird tickets. QCon San Francisco Explore insights, real-world best practices and solutions in software development & leadership. Register now. QCon AI New York Learn how leading engineering teams run AI in production-reliably, securely, and at scale. Register now. InfoQ Homepage News OpenAI Announces GPT-3 AI Language Model with 175 Billion Parameters AI, ML & Data Engineering OpenAI Announces GPT-3 AI Language Model with 175 Billion Parameters This item in japanese Jun 02, 2020 3 									min read by Anthony Alford Write for InfoQ Feed your curiosity. Help 550k+ global senior developers each month stay ahead. Get in touch Like Reading list A team of researchers from OpenAI recently published a paper describing GPT-3, a deep-learning model for natural-language with 175 billion parameters, 100x more than the previous version, GPT-2. The model is pre-trained on nearly half a trillion words and achieves state-of-the-art performance on several NLP benchmarks without fine-tuning. In paper published on arXiv, a team of over 30 co-authors described the model and several experiments. The researchers' goal was to produce an NLP system that performs well on a variety of tasks with little or no fine-tuning, and previous work had indicated that larger models might be the solution. To test that hypothesis, the team increased the size of their previous model, GPT-2 , from 1.5 billion parameters to 175 billion. For training, the team collected several datasets, including the Common Crawl dataset and the English-language Wikipedia . The model was evaluated against several NLP benchmarks, matching state-of-the-art performance on "closed-book" question-answering tasks and setting a new record for the LAMBADA language modeling task. OpenAI made headlines last year with GPT-2 and their decision not to release the 1.5 billion parameter version of the trained model due to "concerns about malicious applications of the technology." GPT-2 is one of many large-scale NLP models based on the Transformer architecture. These models are pre-trained on large text corpora, such as the contents Wikipedia, using self-supervised learning. In this scenario, instead of using a dataset containing inputs paired with expected outputs, the model is given a sequence of text with words "masked" and it must learn to predict the masked words based on the surrounding context. After this pre-training, the models are then fine-tuned with a labelled benchmark dataset for a particular NLP task, such as question-answering. However, researchers have found that the pre-trained models perform fairly well even without fine-tuning, especially for large models pre-trained on large datasets. Earlier this year, OpenAI published a paper postulating several " laws of scaling " for Transformer models. Based on performance data from several different Transformer-based models, OpenAI concluded that model performance (in this case, the cross-entropy loss on the test dataset) has a power-law relationship with the number of model parameters, the size of the dataset, and the amount of compute used for training. Increasing any those three variables would thus improve performance. For pre-training, the team collected a dataset composed of Common Crawl, WebText , English-language Wikipedia, and two corpora of books. To improve data quality, the researchers filtered Common Crawl to remove redundancies. Because Common Crawl is scraped from the internet, it may contain the actual test data for the benchmark evaluations, which would "taint" the training. The team did attempt to remove this contamination; however, they admit: Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. The team used this data to train eight versions of the model, ranging in size from 125 million parameters to the full 175 billion. The models were evaluated on dozens of NLP benchmarks, in a wide range of categories, with performance near or above state-of-the-art in many cases. To evaluate the model on a news-article generation task, the team used Amazon Mechanical Turk to hire human judges to guess which of a pair of articles was real and which was generated by GPT-3. Humans chose the real article only 52% of the time; in essence, humans were no better than a coin-flip at choosing the real article. The team also discussed some weaknesses of the model. For example, on text synthesis, "GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs." The model also has difficulty with "common sense physics" questions, such as, "If I put cheese into the fridge, will it melt?" Several members of the NLP-research community have commented on Twitter about the model's size. Alchemy API founder Elliot Turner speculated that the cost to train the largest model could be "nearly $12 million dollars." Prof. Mark Riedl suggests an explanation for the link between model size and performance: One hypothesis is that GPT-3 has so many parameters (half the number of tokens trained on) that it is starting to act like a memory network. As with GPT-2, OpenAI has not released the trained model or the code, although there is a GitHub repository containing some of the test datasets as well as a collection of text samples generated by the model. Rate this Article Adoption Style Author Contacted This content is in the AI, ML & Data Engineering topic Related Topics: AI, ML & Data Engineering Natural Language Processing Neural Networks Deep Learning Artificial Intelligence Machine Learning Related Editorial Related Sponsored Content Popular across InfoQ LittleHorse, a Java Workflow Engine for Distributed Systems Orchestration Google Go Module Mirror Served Backdoor for 3+ Years InfoQ Software Architecture and Design Trends Report - 2025 Azure MCP Server Enters Public Preview: Expanding AI Agent Capabilities Implement the EU Cyber Resilience Act's Requirements to Strengthen Your Software Project InfoQ Architecture and Design Trends in 2025 Related Content The InfoQ Newsletter A round-up of last week’s content on InfoQ sent out every Tuesday. Join a community of over 250,000 senior developers. View an example Enter your e-mail address Select your country Select a country I consent to InfoQ.com handling my data as explained in this Privacy Notice . We protect your privacy. Development Scaling API Independence: Akehurst on Mocking, Contract Testing, and Observability Activision Reduces Build Time of Call of Duty by 50% with MSVC Build Insights Fast Eventual Consistency: Inside Corrosion, the Distributed System Powering Fly.io Architecture & Design InfoQ Software Architecture and Design Trends Report - 2025 InfoQ Architecture and Design Trends in 2025 AWS Promotes Responsible AI in the Well-Architected Generative AI Lens Culture & Methods A Game of Patterns How Developers Can Eliminate Software Waste and Reduce Climate Impact Building Empathy and Accessibility: Fostering Better Engineering Cultures and Developer Experiences AI, ML & Data Engineering Docker Bridges Agents and Containers with New MCP Catalog and Toolkit Google's Gemma 3 QAT Language Models Can Run Locally on Consumer-Grade GPUs Google DeepMind Shares Approach to AGI Safety and Security DevOps GitHub Announces Public Preview of GitHub MCP Server Docker Desktop 4.40 Introduces Model Runner to Run LLMs Locally, Expanding its AI Capabilities Addressing Kubernetes Authorization with Cedar The InfoQ Newsletter A round-up of last week’s content on InfoQ sent out every Tuesday. Join a community of over 250,000 senior developers. View an example Get a quick overview of content published on a variety of innovator and early adopter technologies Learn what you don’t know that you don’t know Stay up to date with the latest information from the topics you are interested in Enter your e-mail address Select your country Select a country I consent to InfoQ.com handling my data as explained in this Privacy Notice . We protect your privacy. June 9-10, 2025 | Boston, Massachusetts Join senior software developers for two days of actionable insights and real-world solutions to today's toughest engineering challenges. With 27+ talks, you'll hear from trusted practitioners on topics like AI-assisted delivery, scalable architectures, modern data platforms, micro-frontends, and more. Learn from active senior software developers at Google, AWS, MongoDB, Thoughtworks, and beyond — alongside attendees from IBM, Disney, Klaviyo, Reddit, Duolingo, N26, and other leading teams. Register Now Home Create account Login QCon Conferences Events Write for InfoQ InfoQ Editors About InfoQ About C4Media Media Kit InfoQ Developer Marketing Blog Diversity Events InfoQ Dev Summit Boston June 9-10, 2025 InfoQ Dev Summit Munich October 15-16, 2025 QCon San Francisco November 17-21, 2025 / In-person QCon AI New York December 16-17, 2025 Follow us on Youtube 232K Followers Linkedin 26K Followers RSS 19K Readers X 57.1k Followers Facebook 21K Likes Bluesky New Alexa New Stay in the know The InfoQ Podcast Engineering Culture Podcast The Software Architects' Newsletter General Feedback feedback@infoq.com Advertising sales@infoq.com Editorial editors@infoq.com Marketing marketing@infoq.com InfoQ.com and all content copyright © 2006-2025 C4Media Inc. Privacy Notice , Terms And Conditions , Cookie Policy BT