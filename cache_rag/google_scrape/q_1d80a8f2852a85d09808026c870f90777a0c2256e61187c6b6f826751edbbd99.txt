OpenAI Presents GPT-3, a 175 Billion Parameters Language Model | NVIDIA Technical Blog DEVELOPER Home Blog Forums Docs Downloads Training Join Related Resources Conversational AI English 中文 OpenAI Presents GPT-3, a 175 Billion Parameters Language Model Jul 07, 2020 By Nefi Alarcon Like Discuss (0) L T F R E OpenAI researchers recently released a paper describing the development of GPT-3 , a state-of-the-art language model made up of 175 billion parameters. For comparison, the previous version, GPT-2, was made up of 1.5 billion parameters. The largest Transformer-based language model was released by Microsoft earlier this month and is made up of 17 billion parameters. “GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic,” the researchers stated in their paper . “We find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans.” Natural language processing tasks range from generating news articles, to language translation, to answering standardized test questions. Figure 1. Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models which we trained. All models were trained for a total of 300 billion tokens. Source: OpenAI . “The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU’s,” the organization stated . “All models were trained on NVIDIA V100 GPUs on part of a high-bandwidth cluster provided by Microsoft.” OpenAI trains all of their AI models on the cuDNN-accelerated PyTorch deep learning framework. Earlier this month Microsoft and OpenAI announced a new GPU-accelerated supercomputer built exclusively for the organization. “The supercomputer developed for OpenAI is a single system with more than 285,000 CPU cores, 10,000 GPUs and 400 gigabits per second of network connectivity for each GPU server,” the companies stated in a blog . In terms of performance, the new GPT-3 model achieves near state-of-the-art results on the SuperGLUE benchmark, introduced last year to test reasoning and other advanced NLP tasks. In other benchmarks, including COPA and ReCoRD, the model falls short with word-in-context analysis (WIC) and RACE, a set of middle and high school exam questions. “Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems,” the organization said. Read more>> Related resources GTC session: Open-Sora: Training a Commercial-Level Video Generation Model with $200k GTC session: Optimize Parallelism in Large Language Models Training: Trends and Future Directions GTC session: Zyda-2: A 5-Trillion-Token, High-Quality Dataset Built Using NeMo Curator NGC Containers: Llama-3-Taiwan-70B-Instruct NGC Containers: Phind-CodeLlama-34B-v2-Instruct SDK: NGC Models Discuss (0) Like Tags Conversational AI | News | Speech AI | Speech Recognition / Diarization About the Authors About Nefi Alarcon Nefi Alarcon is a senior executive communications manager on NVIDIA's leadership team. He has years of media relations and communication experience, and has previously worked at Google, Mozilla, and CNN. He received his bachelor's degree in Journalism from George Washington University. View all posts by Nefi Alarcon Comments Related posts NVIDIA GB200 NVL72 Delivers Trillion-Parameter LLM Training and Real-Time Inference NVIDIA GB200 NVL72 Delivers Trillion-Parameter LLM Training and Real-Time Inference New Video: What Runs ChatGPT? New Video: What Runs ChatGPT? Increasing Inference Acceleration of KoGPT with NVIDIA FasterTransformer Increasing Inference Acceleration of KoGPT with NVIDIA FasterTransformer Scaling Language Model Training to a Trillion Parameters Using Megatron Scaling Language Model Training to a Trillion Parameters Using Megatron Microsoft Trains Turing-NLG, World’s Largest Transformer Language Model Microsoft Trains Turing-NLG, World’s Largest Transformer Language Model Related posts Spotlight: Personal AI Brings AI Receptionists to Small Business Owners with NVIDIA Riva Spotlight: Personal AI Brings AI Receptionists to Small Business Owners with NVIDIA Riva Structuring Applications to Secure the KV Cache Structuring Applications to Secure the KV Cache Choosing Your First Local AI Project Choosing Your First Local AI Project How SETI Uses AI to Search for Intelligent Alien Life How SETI Uses AI to Search for Intelligent Alien Life Advancing Cybersecurity Operations with Agentic AI Systems Advancing Cybersecurity Operations with Agentic AI Systems L T F R E